{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/ods_stickers.jpg\" />\n",
    "    \n",
    "# Introducción al Machine Learning 2020\n",
    "\n",
    "Basado en material de Pedro Pury y  Sebastian Raschka (sraschka@wisc.edu) Traducido y editado al español por [Ana Georgina Flesia](https://www.linkedin.com/in/georginaflesia/). Este material esta sujeto a los términos y condiciones de la licencia  [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Se permite el uso irrestricto para todo propósito no comercial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Un clasificador se entrena para discriminar los ejemplos en tres\n",
    "clases (A, B y C) y en la evaluación sobre el conjunto de test\n",
    "se obtiene la siguiente matriz de confusión:\n",
    "\n",
    "        ------------------------------\n",
    "                        clasificado\n",
    "                       A    B     C    \n",
    "        ------------------------------\n",
    "                  A |  9    3     1\n",
    "    etiquetado    B |  4    8     2\n",
    "                  C |  2    1     6\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calcular la proporción de aciertos obtenidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la matriz de confusión obtenemos los aciertos de la diagonal, por lo tanto tenemos:\n",
    "\n",
    "$\\hat{p} = \\dfrac{9 + 8 + 6}{36} = \\dfrac{23}{36} = 0.6389$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)  Calcular un intervalo de confianza del $95\\%$ para\n",
    "la métrica de exactitud.\n",
    "Justificar si es válida la aproximación normal para el cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que:\n",
    "\n",
    "* $n = 36$\n",
    "* $\\hat{p} = 0.6389$\n",
    "* $1 - \\hat{p} = \\hat{q}= 0.3611$\n",
    "* $n\\hat{p} = 36 \\cdot 0.6389 =23$\n",
    "* $n\\hat{q} = 36 \\cdot 0.3611 =13$\n",
    "* $z_{\\alpha/2} = 1.96$\n",
    "\n",
    "Como se da que $n\\hat{p} = 23 > 10$ y $n\\hat{q} = 13 > 10$ podemos usar la aproximación normal de modo que:\n",
    "\n",
    "\n",
    "$ IC(95\\%) = \\dfrac{\\hat{p}+ \\dfrac{z_{\\alpha/2}^{2}}{2n} \\pm \\sqrt{\\hat{p}(1-\\hat{p})\\dfrac{z_{\\alpha/2}^{2}}{n} + \\dfrac{z_{\\alpha/2}^{4}}{4n^{2}}}}{1 + \\dfrac{z_{\\alpha/2}^{2}}{n}} = 0.6252 \\pm 0.1498$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6252\n",
      "0.1498\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "p_hat = 0.6386 \n",
    "n = 36\n",
    "q_hat = 1 - p_hat\n",
    "z_alpha = 1.96\n",
    "\n",
    "sq = np.sqrt(p_hat*q_hat*((z_alpha**2)/n) + (z_alpha**4)/(4*(n**2)))\n",
    "sq = sq / (1 + (z_alpha**2)/n)\n",
    "\n",
    "m = p_hat + (z_alpha**2)/(2*n)\n",
    "m = m / (1 + (z_alpha**2)/n)\n",
    "print(round(m, 4))\n",
    "print(round(sq, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)  Calcular el coeficiente $\\kappa$ de Cohen.\n",
    "¿Qué grado de confiabilidad tiene el clasificador?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\kappa = \\dfrac{p_{0} - p_{e}}{1 - p_{e}}$\n",
    "\n",
    "donde $p_{0}:$ proporción de acuerdo observado y $p_{e}:$ proporción de acuerdo independiente\n",
    "\n",
    "De lo anterior nos queda que: $p_{0} = \\hat{p} = 0.6389$\n",
    "\n",
    "Para calcular $p_{e}$:\n",
    "\n",
    "* $p_{Etiq}(A) = 13/36$\n",
    "* $p_{Etiq}(B) = 14/36$\n",
    "* $p_{Etiq}(C) = 9/36$\n",
    "* $p_{Clas}(A) = 15/36$\n",
    "* $p_{Clas}(B) = 12/36$\n",
    "* $p_{Clas}(C) = 9/36$\n",
    "\n",
    "$p_{e} = p_{Etiq}(A)p_{Clas}(A) + p_{Etiq}(B)p_{Clas}(B) + p_{Etiq}(C)p_{Clas}(C) = \\dfrac{13 \\cdot 15 + 14 \\cdot 12 + 9 \\cdot 9}{36^{2}} = 0.3426$ \n",
    "\n",
    "Y queda que:\n",
    "\n",
    "$\\kappa = \\dfrac{p_{0} - p_{e}}{1 - p_{e}} = \\dfrac{0.6389 - 0.3426}{1 - 0.3426} = 0.4507$\n",
    "\n",
    "El clasificador tiene confiabilidad **moderada** en el acuerdo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta segunda parte de esta tarea, combinará múltiples árboles de decisión dentro de un clasificadorde Bagging. Esta vez, utilizaremos el algoritmo del árbol de decisión `DecisionTreeClassifier` implementado en scikit-learn (que es una variante del algoritmo CART para divisiones binarias).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bootsrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging depende del muestreo bootstrap. Entonces, como primer paso, su tarea es implementar una función para generar muestras de arranque. En este ejercicio, por simplicidad, realizaremos los cálculos basados en el conjunto de datos de Iris.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 150\n",
      "Number of features: 4\n",
      "Unique class labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "import numpy as np\n",
    "from mlxtend.data import iris_data\n",
    "X, y = iris_data()\n",
    "\n",
    "print('Number of examples:', X.shape[0])\n",
    "print('Number of features:', X.shape[1])\n",
    "print('Unique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función de scikit-learn `train_test_split` \n",
    "función para dividir el conjunto de datos en un conjunto de entrenamiento y prueba.\n",
    "\n",
    "\n",
    "- El conjunto de prueba debe contener 45 ejemplos, y el conjunto de entrenamiento debe contener 105 ejemplos.\n",
    "- Para garantizar resultados reproducibles, utilice '123' como semilla aleatoria.\n",
    "- Realice una división estratificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=45, \n",
    "                                                    random_state=123,\n",
    "                                                    stratify=y)\n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregué la siguiente celda de código para su conveniencia para verificar su solución. Si sus resultados no coinciden con los resultados que se muestran a continuación, hay un error en su implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])\n",
    "\n",
    "\n",
    "#Number of training examples: 105\n",
    "#Number of test examples: 45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, estamos implementando una función para generar muestras bootstrap del conjunto de entrenamiento. En particular, realizaremos el bootstrapping de la siguiente manera:\n",
    "\n",
    "- Cree una matriz de índice con valores 0, ..., 104.\n",
    "- Extraiga una muestra aleatoria (con reemplazo) de esta matriz de índices utilizando el método  `choice` del objeto `RandomState` de  NumPy  que se pasa a la función como rng.\n",
    "- Seleccione ejemplos de entrenamiento de la matriz X y etiquetas del vector y utilizando la nueva muestra de índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap_sample(rng, X, y):\n",
    "    sample_indices = [i for i in range(0, 105)]\n",
    "    bootstrap_indices = rng.choice(sample_indices, replace=True,\n",
    "                                   size=105)\n",
    "    return X[bootstrap_indices], y[bootstrap_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Agregué la siguiente celda de código para su conveniencia para verificar su solución. Si sus resultados no coinciden con los resultados que se muestran a continuación, hay un error en su implementación de la función `draw_bootstrap_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs from bootstrap round: 105\n",
      "Number of training labels from bootstrap round: 105\n",
      "Labels:\n",
      " [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
      " 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
      " 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "\n",
    "print('Number of training inputs from bootstrap round:', X_boot.shape[0])\n",
    "print('Number of training labels from bootstrap round:', y_boot.shape[0])\n",
    "print('Labels:\\n', y_boot)\n",
    "\n",
    "#Number of training inputs from bootstrap round: 105\n",
    "#Number of training labels from bootstrap round: 105\n",
    "#Labels:\n",
    "# [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
    "# 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
    "# 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clasificador Baggging a partir de árboles de decisión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En esta sección, Ud implementará un algoritmo de bagging basado en `DecisionTree Classifier`. Como ayuda se proporciona una solución parcial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class BaggingClassifier(object):\n",
    "    \n",
    "    def __init__(self, num_trees=10, random_state=123):\n",
    "        self.num_trees = num_trees\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [DecisionTreeClassifier(random_state=self.rng) for i in range(self.num_trees)]\n",
    "        for i in range(self.num_trees):\n",
    "            X_boot, y_boot = draw_bootstrap_sample(self.rng, X, y)\n",
    "            self.trees_[i].fit(X_boot, y_boot)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ary = np.zeros((X.shape[0], len(self.trees_)), dtype=np.int)\n",
    "        for i in range(len(self.trees_)):\n",
    "            ary[:, i] = self.trees_[i].predict(X)\n",
    "\n",
    "        maj = np.apply_along_axis(lambda x:\n",
    "                                  np.argmax(np.bincount(x)),\n",
    "                                            axis=1,\n",
    "                                            arr=ary)\n",
    "        return maj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregué la siguiente celda de código para su conveniencia para verificar su solución. Si sus resultados no coinciden con los resultados que se muestran a continuación, hay un error en su implementación de la función `BaggingClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Tree Accuracies:\n",
      "88.9%\n",
      "93.3%\n",
      "97.8%\n",
      "93.3%\n",
      "93.3%\n",
      "93.3%\n",
      "91.1%\n",
      "97.8%\n",
      "97.8%\n",
      "97.8%\n",
      "\n",
      "Bagging Test Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "model = BaggingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('Individual Tree Accuracies:')\n",
    "for tree in model.trees_:\n",
    "    predictions = tree.predict(X_test) \n",
    "    print('%.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "\n",
    "print('\\nBagging Test Accuracy: %.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "\n",
    "#Individual Tree Accuracies:\n",
    "#88.9%\n",
    "#93.3%\n",
    "#97.8%\n",
    "#93.3%\n",
    "#93.3%\n",
    "#93.3%\n",
    "#91.1%\n",
    "#97.8%\n",
    "#97.8%\n",
    "#97.8%\n",
    "\n",
    "#Bagging Test Accuracy: 97.8%\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Descomposición de sesgo-varianza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este ejercicio se le pedirá que calcule los componentes de varianza y sesgo de la función de pérdida 0-1 que discutimos en clase.\n",
    "\n",
    "- En particular, calculará el sesgo promedio y la varianza promedio sobre todos los ejemplos de prueba (en lugar de un solo ejemplo de prueba).\n",
    "\n",
    "- El conjunto de datos que utilizará como conjunto de entrenamiento y conjunto de prueba es el conjunto de datos de Iris que ya dividió en `X_train` /` y_train` y `X_test` /` y_test` anteriormente.\n",
    "\n",
    "- Dado que no tenemos conjuntos de datos de entrenamiento ilimitados para estimar los parámetros, utilizaremos bootstrapping para simular \"nuevos\" conjuntos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Descomposición de sesgo-varianza  de la pérdida 0-1 para árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "En esta primera parte, calculará los  sesgo y varianza promedios sobre los ejemplos del conjunto de prueba para el algoritmo del árbol de decisión implementado en scikit-learn en los datos de Iris.\n",
    "\n",
    "Ya implementé el código para calcular la \"predicción principal\" para usted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 2 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 1 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 2 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 2 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 2 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 2 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 2 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 2 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 1 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 2 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 2 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 1 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 1 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 2 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 1 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 1 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 1 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 2 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 2]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 1 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 1 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 1 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 1 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 1 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 1 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 2 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 2 1 1 1 2\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 2 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 2 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[2 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 2 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 1 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 2 2 0 2 1 1 2 1]\n",
      "[1 2 1 1 2 0 0 1 0 0 2 0 2 0 1 0 2 2 0 0 0 0 0 1 0 1 2 0 1 2 2 2 1 1 1 1 1\n",
      " 1 2 0 2 1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "num_bootstrap = 200\n",
    "\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = DecisionTreeClassifier(random_state=66).fit(X_boot, y_boot).predict(X_test)\n",
    "    print(pred)\n",
    "    all_pred[i] = pred\n",
    "\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x:\n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que `all_pred` es una matriz 2D de dimensión $\\mathbb{R}^{m \\times n_{test}}$, donde $m$ es el número de iteraciones bootstrap  y $n_{test}$ es el número de ejemplos en el conjunto de prueba. En otras palabras, cada una de las 200 filas de esta matriz almacena las predicciones de una hipótesis de árbol de decisión en particular para los 45 puntos de datos de prueba.\n",
    "\n",
    "Su primera tarea es calcular el sesgo promedio sobre todos los ejemplos de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bias: 0.022\n"
     ]
    }
   ],
   "source": [
    "def calculate_bias(true_labels, main_predictions):\n",
    "    biases = (true_labels != main_predictions).astype(int)\n",
    "    bias = round(np.count_nonzero(biases)/true_labels.shape[0], 3)\n",
    "    return bias\n",
    "\n",
    "bias = calculate_bias(y_test, main_predictions)\n",
    "print('Average bias:', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su segunda tarea es calcular la varianza promedio sobre todos los ejemplos de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average variance: 0.035\n"
     ]
    }
   ],
   "source": [
    "def calculate_variance(all_pred, main_predictions):\n",
    "    variance = (all_pred != main_predictions).astype(int)\n",
    "    var = round(np.count_nonzero(variance)/variance.size, 3)\n",
    "    return var\n",
    "\n",
    "var = calculate_variance(all_pred, main_predictions)\n",
    "print('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sugerencia: Los valores promedio de sesgo y varianza son escalares, no vectores o matrices. En otras palabras, para cada una de las celdas de código anteriores, debe devolver un número real (float)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Descomposición de sesgo-varianza  de la pérdida 0-1 para Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilice el código de la sección anterior, 3.1, para comparar el algoritmo del árbol de decisión con un BaggingClassifier de scikit-learn.\n",
    "\n",
    "- Informe tanto el sesgo promedio como la varianza promedio como antes, pero use el `BaggingClassifier` en scikit-learn en lugar del` DecisionTreeClassifier`. Puede usar los valores predeterminados de `BaggingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bias: 0.022\n",
      "Average variance: 0.027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "num_bootstrap = 200\n",
    "\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = BaggingClassifier(n_estimators=100, random_state=66).fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x:\n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)\n",
    "\n",
    "\n",
    "bias = calculate_bias(y_test, main_predictions)\n",
    "var = calculate_variance(all_pred, main_predictions)\n",
    "\n",
    "print('Average bias:', bias)\n",
    "print('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es la varianza promedio de bagging mayor o menor que la varianza promedio del árbol de decisión en 3.1? ¿Y qué \n",
    "hay del sesgo promedio? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La varianza promedio es menor que la del árbol de decisión, mientras que la el bias promedio permanece igual. La menor varianza indica que el clasificador bagging tiene menor tendencia a sobreajustar los datos que el arbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Descomposición de sesgo-varianza  de la pérdida 0-1 para  AdaBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Utilice el código de la sección anterior, 3.1, para comparar el algoritmo del árbol de decisión con un AdaBoostClassifier de scikit-learn.\n",
    "\n",
    "- Informe tanto el sesgo promedio como la varianza promedio como antes, pero use el `AdaboostClassifier` en scikit-learn en lugar del` DecisionTreeClassifier`. Puede usar los valores predeterminados de `AdaboostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bias: 0.044\n",
      "Average variance: 0.033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "num_bootstrap = 200\n",
    "\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = AdaBoostClassifier(n_estimators=30,\n",
    "                              random_state=123).fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x:\n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)\n",
    "\n",
    "bias = calculate_bias(y_test, main_predictions)\n",
    "var = calculate_variance(all_pred, main_predictions)\n",
    "\n",
    "print('Average bias:', bias)\n",
    "print('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Es la varianza promedio de boosting mayor o menor que la varianza promedio del árbol de decisión en 3.1? ¿Y qué hay del sesgo promedio? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La varianza promedio disminuyó y el sesgo promedio aumentó en comparación con el árbol de decisión del 3.1. Los modelos que se consideran para ser usados en Bootsing deben tener baja varianza y alto sesgo ('weakers learners'), ya que lo que busca es reducir el sesgo. Si se cambian el *base_estimator* de **AdaBoostClassifier** por un arbol de decisión con pocos niveles (5 ó 4, árbol que no sobreajusta) entonces el metodo mantiene el sesgo y la varianza apenas incrementa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)  Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este ejercicio se lepide que  que aplique un `RandomForestClassifier` en un pequeño subconjunto (10%) del conjunto de datos de dígitos manuscritos MNIST (http://yann.lecun.com/exdb/mnist/). Por conveniencia, el siguiente código carga este pequeño subconjunto a través de mlxtend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 5000 x 784\n",
      "1st row [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  48. 238. 252. 252. 252. 237.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  54.\n",
      " 227. 253. 252. 239. 233. 252.  57.   6.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.  60. 224.\n",
      " 252. 253. 252. 202.  84. 252. 253. 122.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 252. 252.\n",
      " 252. 253. 252. 252.  96. 189. 253. 167.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 238. 253. 253.\n",
      " 190. 114. 253. 228.  47.  79. 255. 168.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 179.\n",
      "  12.  75. 121.  21.   0.   0. 253. 243.  50.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  38. 165. 253. 233. 208.  84.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 165.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   7. 178. 252. 240.  71.  19.  28.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  57. 252. 252.  63.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 198. 253. 190.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 255. 253. 196.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  76. 246. 252. 112.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 148.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 230.  25.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   7. 135. 253. 186.  12.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 223.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   7. 131. 252. 225.  71.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 145.   0.   0.   0.   0.   0.\n",
      "   0.   0.  48. 165. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  86. 253. 225.   0.   0.   0.   0.   0.\n",
      "   0. 114. 238. 253. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 249. 146.  48.  29.  85. 178.\n",
      " 225. 253. 223. 167.  56.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 252. 252. 229. 215. 252. 252.\n",
      " 252. 196. 130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  28. 199. 252. 252. 253. 252. 252. 233.\n",
      " 145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  25. 128. 252. 253. 252. 141.  37.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.data import mnist_data\n",
    "X, y = mnist_data()\n",
    "\n",
    "print('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\n",
    "print('1st row', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El siguiente código muestra como se mezcla el conjunto de datos y se lo divide en 4500 ejemplos de entrenamiento y 500 ejemplos de prueba, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "\n",
    "\n",
    "X, y = shuffle_arrays_unison((X, y), random_seed=1)\n",
    "X_train, y_train = X[:4500], y[:4500]\n",
    "X_test, y_test = X[4500:], y[4500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora, su tarea es ajustar un clasificador RandomForest en el conjunto de entrenamiento y evaluar su precisión predictiva en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 93.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "acc = ((predictions == y_test).sum() / X_test.shape[0] * 100)\n",
    "print('Accuracy %.1f%%' % acc)\n",
    "    \n",
    "#Accuracy 93.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Luego, su tarea es cargar una imagen de un dígito (some_digit.png) de este directorio en una matriz de Python y clasificarla usando el modelo de bosque aleatorio. La imagen some_digit.png se muestra a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/some_digit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nota: Para cargar la imagen, debe instalar la biblioteca de imágenes Python PIL. En realidad, es mas recomendable instalar  Pillow. Ejecute uno de los siguientes dos si aún no ha instalado Pillow.\n",
    "    \n",
    "- `conda install Pillow`\n",
    "\n",
    "- `pip install Pillow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, preescribí parcialmente el código para usted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_image(file_name):\n",
    "    img = Image.open(file_name)\n",
    "    img.load()\n",
    "    data = np.asarray(img, dtype=np.float)\n",
    "    return data\n",
    "\n",
    "x_image = load_image('./images/some_digit.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 5\n"
     ]
    }
   ],
   "source": [
    "# The data needs to be represented as a vector (1 position for each feature)\n",
    "x_transf = x_image.reshape(1, -1)[0]\n",
    "\n",
    "# Also, scikit-learn expects 2D arrays, so we need to add a dimension\n",
    "x_transf = x_transf[np.newaxis, :]\n",
    "\n",
    "print('Digit:', model.predict(x_transf)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
