{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/ods_stickers.jpg\" />\n",
    "    \n",
    "# Introducción al Machine Learning 2020\n",
    "\n",
    "Basado en material de Pedro Pury y  Sebastian Raschka (sraschka@wisc.edu) Traducido y editado al español por [Ana Georgina Flesia](https://www.linkedin.com/in/georginaflesia/). Este material esta sujeto a los términos y condiciones de la licencia  [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Se permite el uso irrestricto para todo propósito no comercial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Árboles de decision\n",
    "\n",
    "Al entrar al bar, sus **cinco** amigos, quienes se reúnen al salir\n",
    "de trabajar, ya tiene sus bebidas sobre la mesa.\n",
    "María que tiene un cargo de gerente paga la ronda la mitad de las\n",
    "veces. Pablo paga la ronda la cuarta parte de las veces, mientras\n",
    "que Sara y Carlos que son becarios pagan indistintamente entre ambos\n",
    "la octava parte de las veces. Juan nunca sacó la billetera desde\n",
    "que se reúnen los viernes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)  ¿Qué fracción de las veces ud. paga la ronda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero obtenemos las probabilidades de pagar de cada uno de mis amigos:\n",
    "\n",
    "$p_{Maria} = \\dfrac{1}{2}$, $p_{Sara} = \\dfrac{1}{16}$, $p_{Carlos} = \\dfrac{1}{16}$, $p_{Pablo} = \\dfrac{1}{4}$, \n",
    "$p_{Juan} = 0$ \n",
    "\n",
    "Luego la probabilidad ($p_{}$) de que yo deba pagar es:\n",
    "\n",
    "$p_{} = 1 - p_{Maria} - p_{Pablo} - p_{Sara} - p_{Carlos} -p_{Juan} = 1 - \\dfrac{1}{2} - \\dfrac{1}{4} - \\dfrac{1}{16} - \\dfrac{1}{16} - 0 = \\dfrac{1}{8}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calcular la entropía de la distribución de probabilidad\n",
    "con la que cada uno paga la ronda. ¿Cuál es el número medio\n",
    "de preguntas (de respuesta binaria) que necesitan hacerse para\n",
    "saber quién paga la ronda? [articulo](https://nusgrem.es/informacion-entropia-probabilidad/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Entropy(P) = \\sum -p_{i}\\log_{2}p_{i} = -\\dfrac{1}{2}\\log_2 \\dfrac{1}{2} - \\dfrac{1}{4}\\log_2 \\dfrac{1}{4} - \\dfrac{2}{16}\\log_2 \\dfrac{1}{16} - 0 \\log_2 0 - \\dfrac{1}{8} \\log_2 \\dfrac{1}{8} = \\dfrac{1}{2} + \\dfrac{2}{4} + \\dfrac{8}{16} + 0 + \\dfrac{3}{8} = 1.875$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poco después de arribar al bar se suman dos antiguos amigos quienes\n",
    "no vivieron en la ciudad el último año. Ellos deciden que la próxima\n",
    "ronda la debe pagar ud. y conociendo que cursa esta materia,\n",
    "lo desafían a predecir que bebida tomará cada uno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Usando la información de las tres variables binarias:\n",
    "sexo, si es o no estudiante y si le gusta o no bailar,\n",
    "y recordando la elección de bebidas de la noche anterior,\n",
    "puede construirse la siguiente tabla:\n",
    "\n",
    "\n",
    "    |Bebida | Género | Estudiante | Baile |\n",
    "    --------------------------------------------\n",
    "    cerveza | M      | T          | T  \n",
    "    cerveza | M      | F          | T  \n",
    "    vodka   | M      | F          | F  \n",
    "    vodka   | M      | F          | F  \n",
    "    vodka   | F      | T          | T  \n",
    "    vodka   | F      | F          | F  \n",
    "    vodka   | F      | T          | T \n",
    "    vodka   | F      | T          | T \n",
    "    -------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Usando entropía de información entrenar un árbol\n",
    "de decisión a partir de la tabla anterior.\n",
    "Registrar todos los valores calculados para elegir las variables\n",
    "en cada nodo del árbol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sea $S$ la colección de ejemplos de entrenamiento de la tabla. Para correr ID3 tenemos que calcular la ganancia de información ($Gain(S, A)$) para cada uno de los atributos $A$ (*Género*, *Estudiante* y *Baile*) y el que tenga la mayor ganancia.\n",
    "\n",
    "Primero calculamos la entropía de $S$ respecto del concepto *Cerveza*. Tenemos 2 ejemplos positvos y 6 negativos, de un total de 8. Luego :\n",
    "\n",
    "$Entropy([2+, 6-]) = -\\dfrac{2}{8}\\log_2 \\dfrac{2}{8} - \\dfrac{6}{8} \\log_2 \\dfrac{6}{8} = -\\dfrac{1}{4}\\log_2 \\dfrac{1}{4} - \\dfrac{3}{4} \\log_2 \\dfrac{3}{4} = \\dfrac{1}{2} + 0.311 = 0.811$ \n",
    "\n",
    "\n",
    "Ahora podemos calcular las ganancias de información para cada atributo:\n",
    "\n",
    "* **Género**\n",
    "\n",
    "$|S_{M}| = 4$ y $|S_{F}| = 4$\n",
    "\n",
    "Luego $Entropy(S_{M}) = Entropy([2+, 2-]) = 1$ y $Entropy(S_{F}) = Entropy([0+, 4-]) = 0$\n",
    "\n",
    "$Gain(S, Genero) = 0.811 - (\\dfrac{4}{8} 1 + \\dfrac{4}{8} 0) = 0.311$\n",
    "\n",
    "* **Estudiante**\n",
    "\n",
    "$|S_{T}| = 4$ y $|S_{F}| = 4$\n",
    "\n",
    "Luego $Entropy(S_{T}) = Entropy(S_{F}) = Entropy([1+, 3-]) = -\\dfrac{1}{4} \\log_2 \\dfrac{1}{4} - \\dfrac{3}{4} \\log_2 \\dfrac{3}{4} = \\dfrac{1}{2} + 0.311 = 0.811$\n",
    "\n",
    "$Gain(S, Estudiante) = 0.811 - (\\dfrac{4}{8} 0.811 + \\dfrac{4}{8} 0.811) = 0.811 - 0.811 = 0$\n",
    "\n",
    "* **Baile**\n",
    "\n",
    "$|S_{T}| = 5$ y $|S_{F}| = 3$\n",
    "\n",
    "Luego $Entropy(S_{T}) = Entropy([2+, 3-]) = -\\dfrac{2}{5} \\log_2 \\dfrac{2}{5} - \\dfrac{3}{5} \\log_2 \\dfrac{3}{5} =  0.529 + 0.442 = 0.971$\n",
    "\n",
    "Por otro lado,  $Entropy(S_{f}) = Entropy([0+, 3-]) = 0$\n",
    "\n",
    "$Gain(S, Baile) = 0.811 - (\\dfrac{5}{8} 0.971 + \\dfrac{3}{8} 0) = 0.811 - 0.607 = 0.204$\n",
    "\n",
    "Según las ganancias calculadas el atributo *Género* provee la mejor predicción del atributo *Cerveza*, por lo tanto se elige **Género** como el nodo raíz del árbol. De la raíz salen dos ramas $F$ y $M$; $F$ tiene como hoja a $Vodka$ mientras que con $M$ se tiene que agregar otro nodo porque tenemos 4 ejemplos, 2 con *Cerveza* y 2 con *Vodka*.\n",
    "\n",
    "<br> \n",
    "Por lo tanto ahora definimos $S_{1}$ con esos 4 ejemplos y calculamos su entropía teniendo en cuenta el concepto *Cerveza*.\n",
    "\n",
    "$Entropy(S_{1}) = Entropy([2+, 2-]) = 1$\n",
    "\n",
    "Ahora calculamos la gananancia con respecto en los conceptos que quedan:\n",
    "\n",
    "* **Baile**\n",
    "\n",
    "$|S_{{1}_T}| = 2$ y $|S_{{1}_F}| = 2$\n",
    "\n",
    "Luego $Entropy(S_{{1}_T}) = Entropy([2+, 0-]) = 0$ y $Entropy(S_{{1}_F}) = Entropy([0, 2-]) = 0$\n",
    "\n",
    "Por lo tanto $Gain(S_{1}, Baile) = 1 - (\\dfrac{2}{4} 0 + \\dfrac{2}{4} 0) = 1$\n",
    "\n",
    "* **Estudiante**\n",
    "\n",
    "$|S_{{1}_T}| = 1$ y $|S_{{1}_F}| = 3$\n",
    "\n",
    "Luego $Entropy(S_{{1}_T}) = Entropy([1+, 0-]) = 0$ \n",
    "\n",
    "También, $Entropy(S_{{1}_F}) = Entropy([1+, 2-]) = -\\dfrac{1}{3} \\log_2 \\dfrac{1}{3} - -\\dfrac{2}{3} \\log_2 \\dfrac{2}{3} = 0.528 + 0.39 = 0.918$\n",
    "\n",
    "Por lo tanto $Gain(S_{1}, Estudiante) = 1 - (\\dfrac{1}{4} 0 + \\dfrac{3}{4} 0.918) = 1 - 0.689 = 0.311$\n",
    "\n",
    "Así que se elige como segundo nodo **Baile** del cual saldrán dos ramas: la rama $T$ y $F$, la primera lleva a la hoja $Cerveza$ y la segunda a la hoja $Vodka$. \n",
    "\n",
    "El árbol que nos queda es este:\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/decision_tree_3.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)  Proponer una poda del árbol y justificarla.\n",
    "\n",
    "Solo podemos quitar la rama baile, si hacemos eso perdemos la clasificacion del $100 \\%$ lograda y obtenemos una del $75 \\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Implementing an ID3 Decision Tree ( problem suggested by: Sebastian Raschka (sraschka@wisc.edu)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Splitting a node \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to implement a function that splits a dataset along a feature axis into sub-datasets. Since we are going to implement a decision tree that only supports categorical features (like ID3) for simplicity, you do not need to account for continuous feature variables. In other words, the splitting function only needs to support integer NumPy arrays.  \n",
    "\n",
    "To provide an intuitive example, suppose you are given the following NumPy array with four feature values, feature values 0-3:\n",
    "\n",
    "    np.array([0, 1, 2, 1, 0, 3, 1, 0, 1, 2])\n",
    "    \n",
    "The function you are going to implement should return a dictionary, where each dictionary key represents a unique value in the array, and the values are the indices in that array that map to the respective feature value. Hence, based on the feature array above, your `split` function should return the following dictionary:\n",
    "\n",
    "    {0: array([0, 4, 7]), \n",
    "     1: array([1, 3, 6, 8]), \n",
    "     2: array([2, 9]), \n",
    "     3: array([5])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: I recommend you to use `np.where` and `np.unique` functions to make the implementation easier. If you do not remember these functions from the \"computational foundations\" lectures, you can either look up those functions in the NumPy documentation online, or you can execute `np.where?` and `np.unique?` in a new code cell to get more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(array):\n",
    "    \n",
    "    split_array = {}\n",
    "    unique = np.unique(array)\n",
    "    \n",
    "    for u in unique:\n",
    "        split_array[u] = np.where(array==u)[0]\n",
    "    \n",
    "    return split_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0]), 1: array([1]), 2: array([2])}\n",
      "{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
      "{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(split(np.array([0, 1, 2])))\n",
    "print(split(np.array([1, 0, 1, 0, 0, 1, 0])))\n",
    "print(split(np.array([1, 0, 3, 2, 0, 1, 1])))\n",
    "\n",
    "#{0: array([0]), 1: array([1]), 2: array([2])}\n",
    "#{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
    "#{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2) Implement Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the splitting function, we are now have to implement a criterion function so that we can compare splits on different features, to decide which feature is the best feature to split for growing the decision tree. As discussed in class, our splitting criterion will be Information Gain. However, before we implement an Information Gain function, we need to implement a function that computes the entropy at each node, which we need to compute Information Gain.\n",
    "\n",
    "For your reference, we defined entropy (i.e., Shannon Entropy) as follows:\n",
    "\n",
    "$$H(p) = \\sum_i p_i \\log_2 (1/p_i) = - \\sum_i p_i \\log_2 (p_i)$$\n",
    "\n",
    "where you can think of $p_i$ as the proportion of examples with class label $i$ at a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(array):\n",
    "    n, entropy = len(array), 0\n",
    "    \n",
    "    split_array = split(array)\n",
    "    \n",
    "    for key in split_array.keys():\n",
    "        p_i = len(split_array[key])/n\n",
    "        entropy -= p_i*np.log2(p_i)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `entropy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.4395\n",
      "0.0\n",
      "1.6577\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(round(entropy(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(entropy(np.array([1, 2])), 4))\n",
    "print(round(entropy(np.array([1, 1])), 4))\n",
    "print(round(entropy(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))\n",
    "\n",
    "#1.0\n",
    "#1.0\n",
    "#0.0\n",
    "#0.4395\n",
    "#0.0\n",
    "#1.6577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Implement Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working solution for the `entropy` function, the next step is to compute the Information Gain. For your reference, information gain is computed as\n",
    "\n",
    "$$GAIN(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in Values(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(x_array, y_array):\n",
    "    parent_entropy = entropy(x_array)\n",
    "\n",
    "    split_dict = split(y_array)\n",
    "    \n",
    "    d = len(x_array) \n",
    "    for val in split_dict:\n",
    "        freq = len(split_dict[val])/d\n",
    "        child_entropy = entropy(x_array[split_dict[val]])\n",
    "        parent_entropy -= freq*child_entropy\n",
    "        \n",
    "    return parent_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `information_gain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591\n",
      "0.2516\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x = np.array([0, 1, 0, 1, 0, 1])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))\n",
    "\n",
    "x = np.array([0, 0, 1, 1, 2, 2])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))\n",
    "\n",
    "#0.4591\n",
    "#0.2516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Decision Tree Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have all the main components that we need for implementing the ID3 decision tree algorithm: a `split` function, an `entropy` function, and an `information_gain` function based on the `entropy` function. \n",
    "\n",
    "The next task is combine these functions to recursively split a dataset on its different features to construct a decision tree that separate the examples from different classes well. We will call this function `make_tree`. \n",
    "\n",
    "For simplicity, the decision tree returned by the `make_tree` function will be represented by a Python dictionary. To illustrate this, consider the following dataset:\n",
    "\n",
    "```\n",
    "Inputs:\n",
    " [[0 0]\n",
    " [0 1]\n",
    " [1 0]\n",
    " [1 1]\n",
    " [2 0]\n",
    " [2 1]]\n",
    "\n",
    "Labels:\n",
    " [0 1 0 1 1 1]\n",
    "```\n",
    " \n",
    "This is a dataset with 6 training examples and two features.  The decision tree in form of the Python dictionary should look like as follows:\n",
    "\n",
    "\n",
    "\n",
    "You should return a dictionary with the following form:\n",
    "\n",
    "```\n",
    "{'X_1 = 0': {'X_0 = 0': array([0]),\n",
    "             'X_0 = 1': array([0]),\n",
    "             'X_0 = 2': array([1])},\n",
    " 'X_1 = 1': array([1, 1, 1])}\n",
    " ```\n",
    " \n",
    "Let me further illustrate what the different parts of the dictionary mean. Here, the `'X_1'` in `'X_1 = 0'` refers feature 2 (the first column of the NumPy array; remember that Python starts the index at 0, in contrast to R). \n",
    "\n",
    "- 'X_1 = 0': For training examples stored in this node, the second feature has the value 0\n",
    "- 'X_1 = 1': For training examples stored in this node, the second feature has the value 1\n",
    "\n",
    "The \"array\" is a NumPy array that stores the class labels of the training examples at that node. In the case of `'X_1 = 0'` we actually store actually a sub-dictionary, because this node can be split further. If you have trouble understanding this dictionary representation, the following illustration might help:\n",
    "\n",
    "\n",
    "![](tree-viz-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(X, y):\n",
    "    \n",
    "    # Return array if node is empty or pure (1 example in leaf node)\n",
    "    if y.shape[0] == 1 or y.shape[0] == 0:\n",
    "        return y\n",
    "\n",
    "    # Compute information gain for each feature\n",
    "    gains = np.apply_along_axis(information_gain, 0, X, y)\n",
    "\n",
    "    # Early stopping if there is no information gain\n",
    "    if (gains <= 1e-05).all():\n",
    "        return y\n",
    "    \n",
    "    # Else, get best feature\n",
    "    best_feature = np.argmax(gains)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Use the `split` function to split on the best feature\n",
    "    subset_dict = split(X[:, best_feature])\n",
    "\n",
    "    # Note that each entry in the dictionary returned by \n",
    "    # split is an attribute_value:array_indices pair.\n",
    "    # here, we are going to iterate over these key-value\n",
    "    # pairs and select the respective examples for the\n",
    "    # new child nodes\n",
    "    \n",
    "    for feature_value, train_example_indices in subset_dict.items():\n",
    "        child_y_subset = y[train_example_indices]\n",
    "        child_x_subset = np.delete(X, best_feature, 1)[train_example_indices]\n",
    "\n",
    "        # Next, we are using \"recursion,\" that is, calling the same\n",
    "        # tree_split function on the child subset(s)\n",
    "        \n",
    "        results[\"X_%d = %d\" % (best_feature, feature_value)] = \\\n",
    "                make_tree(child_x_subset, child_y_subset)\n",
    "\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 1]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x1 = np.array([0, 0, 1, 1, 2, 2])\n",
    "x2 = np.array([0, 1, 0, 1, 0, 1])\n",
    "X = np.array([x1, x2]).T\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))\n",
    "\n",
    "#Inputs:\n",
    "# [[0 0]\n",
    "# [0 1]\n",
    "# [1 0]\n",
    "# [1 1]\n",
    "# [2 0]\n",
    "# [2 1]]\n",
    "\n",
    "#Labels:\n",
    "# [0 1 0 1 1 1]\n",
    "\n",
    "#Decision tree:\n",
    "# {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Building a Decision Tree API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of this part of the homework is now to write an API around our decision tree code so that we can use is for making predictions. Here, we will use the common convention, established by scikit-learn, to implement the decision tree as a Python class with \n",
    "\n",
    "- a `fit` method that learns the decision tree model from a training set via the `make_tree` function we already implemented;\n",
    "- a `predict` method to predict the class labels of training examples or any unseen data points.\n",
    "\n",
    "For making predictions, since not all leaf nodes are guaranteed to be single training examples, we will use a majority voting function to predict the class label as discussed in class. I already implemented a `_traverse` method, which will recursively traverse a decision tree dictionary that is produced by the `make_tree` function.\n",
    "\n",
    "Note that for simplicity, the `predict` method will only be able to accept one data point at a time (instead of a collection of data points). Hence `x` is a vector of size $\\mathbb{R}^m$, where $m$ is the number of features. I use capital letters `X` to denote a matrix of size $\\mathbb{R}^{n\\times m}$, where $n$ is the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3DecisionTreeClassifer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.splits_ = make_tree(X, y)\n",
    "        \n",
    "    def _majority_vote(self, label_array):\n",
    "        values,counts = np.unique(label_array,return_counts=True)\n",
    "        ind = np.argmax(counts)\n",
    "        return label_array[ind]\n",
    "        \n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            name, value = key.split(' = ')\n",
    "            feature_idx = int(name.split('_')[-1])\n",
    "            value = int(value)\n",
    "            if x[feature_idx] == value:\n",
    "                return self._traverse(x, d[key])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        label_array = self._traverse(x, self.splits_)\n",
    "        return self._majority_vote(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "tree = ID3DecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict(np.array([0, 0])))\n",
    "print(tree.predict(np.array([0, 1])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 1])))\n",
    "print(tree.predict(np.array([2, 0])))\n",
    "print(tree.predict(np.array([2, 1])))\n",
    "\n",
    "#0\n",
    "#1\n",
    "#0\n",
    "#0\n",
    "#1\n",
    "#1\n",
    "#1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
